
# Part 3 - Open-Answer Questions

#### Q1: How is BERTScore calculated? Read the first three paragraphs in Section 3 -- called "Token representation", "Similarity Measure", and "BERTScore" -- in [this paper](https://arxiv.org/pdf/1904.09675.pdf) and give a technical description of how the BERTScore precision/recall/f1 is calculated in ~6 sentences. You do not need to describe anything outside the scope of these specific paragraphs.

*Your answer here*

#### Q2: How is COMET trained and calculated? Read Section 2.4 -- "Translation Ranking Model" -- in [this paper](https://arxiv.org/pdf/2009.09025.pdf) and give a technical description in ~6 sentences.

*Your answer here*


#### Q3: Given your understanding of BLEU, BERTScore and COMET, how would you interpret the Kendall's Tau correlation results? Which ones are the least and most correlated? What is your hypothesis regarding the reasons behind the lowest correlation for one metric and the highest correlation in the other?

*Your answer here*


#### Q4: Assume you have a large set of story beginnings and you would like to evaluate how well a model completes the stories. What problem would you run into with BLEU and COMET? Would the same disadvantages apply to BERTScore and why? Give your justification. Answer in ~6 sentences.

*Your answer here*
